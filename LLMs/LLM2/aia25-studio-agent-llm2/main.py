from server.config import *
from llm_calls import *
import json

# Placeholder design data
design_data = {
    "materials": ["concrete", "glass", "timber"],
    "embodied_carbon": "420 kgCOâ‚‚e/mÂ²",
    "solar_radiation_area": "380 mÂ²",
    "number_of_levels": 6,
    "typology": "block",  # Options: courtyard, block, L-shaped
    "unit_counts": {
        "3BD": 8,
        "2BD": 12,
        "1BD": 10
    },
    "GFA": "2,400 mÂ²",
    "plot_dimensions": "30m x 40m"
}

# Confirm what model is being used
print("ðŸ”§ Using model:", completion_model)

# Initial prompt
print("\nðŸ‘‹ " + query_intro())

# Loop
while True:
    user_input = input("\nðŸ’¬ What would you like to know about your design? (type 'exit' to quit)\n> ")

    if user_input.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Goodbye!")
        break

    if any(word in user_input.lower() for word in ["improve", "reduce", "maximize", "minimize", "optimize", "should i", "could i", "recommend", "how can i"]):
        suggestion = suggest_improvements(user_input, design_data)
        print("\nðŸ§© Suggestion:\n" + suggestion)
    else:
        reply = answer_user_query(user_input, design_data)
        print("\nðŸ“Š Data Insight:\n" + reply)
